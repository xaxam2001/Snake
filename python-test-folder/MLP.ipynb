{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537f5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfafe67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, npl: List[int]): # neurons per layer, d in the slides\n",
    "        self.npl = list(npl)\n",
    "        self.L = len(npl)-1  # last layer index\n",
    "        self.W = []  # weights weight of layer l going from neuron i to neuron j: W[l][i][j]\n",
    "\n",
    "        for l in range(self.L + 1):\n",
    "            self.W.append([])\n",
    "            if (l == 0) : continue # no weights for input layer, to be consistent with indexing (W[1] are weights going to layer 1)\n",
    "\n",
    "            for i in range(0, self.npl[l-1]+1): # +1 for bias neuron\n",
    "                self.W[l].append([])\n",
    "                for j in range(0, self.npl[l]+1):\n",
    "                    rdm_value = random.random() * 2 - 1  # random value between -1 and 1\n",
    "                    self.W[l][i].append(0.0 if j == 0 else rdm_value) # no weights going to bias neuron so set to 0.0 if j == 0 corresponds to bias neuron\n",
    "\n",
    "        self.X = []  # activations of layer l neuron i: X[l][i]\n",
    "        self.deltas = []  # deltas of layer l neuron i: deltas[l][i]\n",
    "\n",
    "        for l in range(self.L + 1):\n",
    "            self.X.append([])\n",
    "            self.deltas.append([])\n",
    "\n",
    "            for j in range(0, self.npl[l] + 1):\n",
    "                self.X[l].append(1.0 if j==0 else 0.0)\n",
    "                self.deltas[l].append(0.0)\n",
    "\n",
    "    def _propagate(self, inputs: List[float], is_classification: bool):\n",
    "        assert(len(inputs) == self.npl[0]) # check if inputs is the same size as nbs of layers\n",
    "\n",
    "        # update first layer\n",
    "        for j in range(len(inputs)):\n",
    "            self.X[0][j + 1] = inputs[j] # update X for input (layer 0) and keep the weight\n",
    "\n",
    "        # update all layers until output\n",
    "        for l in range(1, self.L + 1):\n",
    "            for j in range(1, self.npl[l]+1):\n",
    "                signal = 0.0\n",
    "                for i in range(0, self.npl[l-1] + 1):\n",
    "                    signal += self.W[l][i][j] * self.X[l - 1][i]\n",
    "                if (is_classification or l != self.L):\n",
    "                    self.X[l][j] = math.tanh(signal)\n",
    "                else:\n",
    "                    self.X[l][j] = signal # use identity method only when regression and updating last layer\n",
    "\n",
    "    def predict(self, inputs: List[float], is_classification: bool):\n",
    "        self._propagate(inputs, is_classification)\n",
    "        return self.X[self.L][1:]\n",
    "    \n",
    "    def train(self,\n",
    "              all_samples_inputs: List[List[float]],\n",
    "              all_samples_expected_outputs: List[List[float]],\n",
    "              is_classification: bool,\n",
    "              num_iter: int,\n",
    "              alpha: float):\n",
    "        \n",
    "        assert(len(all_samples_inputs) == len(all_samples_expected_outputs))\n",
    "        for _ in tqdm(range(num_iter)):\n",
    "            k = random.randint(0, len(all_samples_inputs) - 1)\n",
    "            inputs_k = all_samples_inputs[k]\n",
    "            labels_k = all_samples_expected_outputs[k]\n",
    "\n",
    "            self._propagate(inputs_k, is_classification) # mise à jour des sorties de tous les neurons (self.X)\n",
    "\n",
    "            for j in range(1, self.npl[self.L] + 1): # start with 1 to avoid last layer 1\n",
    "                self.deltas[self.L][j] = (self.X[self.L][j] - labels_k[j-1])\n",
    "                if (is_classification) :\n",
    "                    self.deltas[self.L][j] *= (1.0 - self.X[self.L][j]**2)\n",
    "\n",
    "            # get the deltas\n",
    "            for l in reversed(range(2, self.L + 1)):\n",
    "                for i in range(1, self.npl[l-1] + 1):\n",
    "                    total = 0.0\n",
    "                    for j in range(1, self.npl[l] + 1):\n",
    "                        total += self.W[l][i][j] * self.deltas[l][j]\n",
    "                    total *= (1.0 - self.X[l-1][i] ** 2)\n",
    "                    self.deltas[l-1][i] = total\n",
    "\n",
    "            # update weights\n",
    "            for l in range(1, self.L + 1):\n",
    "                for i in range(0, self.npl[l-1] + 1):\n",
    "                    for j in range(1, self.npl[l] + 1):\n",
    "                        self.W[l][i][j] -= alpha*self.X[l-1][i]*self.deltas[l][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c61a0c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0]]\n",
      "[[], [[0.0, 0.7752035411652591, -0.7216063644188722, -0.6950180558156336], [0.0, -0.38363550086706244, -0.18081422199487762, -0.6628712099644818], [0.0, -0.39070203866735453, -0.1817300188985167, -0.18476663299820162]], [[0.0, 0.3471516069236371], [0.0, 0.1226352783992326], [0.0, 0.5223248264931468], [0.0, -0.0749928541662932]]]\n",
      "[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP([2, 3, 1])\n",
    "print(mlp.X)\n",
    "print(mlp.W)\n",
    "print(mlp.deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ecb75149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.9999997717138139]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict([42.0, 53.0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "868b5c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7960162180703377]\n",
      "[-0.9768997558605246]\n",
      "[-0.9997499759227675]\n",
      "[-0.9999782765544063]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:10<00:00, 9575.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9953337365601457]\n",
      "[0.9944584489935214]\n",
      "[-0.9999999207494784]\n",
      "[-0.9978713134533167]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "xor_inputs = [\n",
    "    [0.0, 0.0],\n",
    "    [1.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 1.0]\n",
    "]\n",
    "\n",
    "xor_expected_ouputs = [\n",
    "    [-1.0],\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [-1.0]\n",
    "]\n",
    "\n",
    "for inp in xor_inputs:\n",
    "    print(mlp.predict(inp, True))\n",
    "mlp.train(xor_inputs, xor_expected_ouputs, True, 100000, 0.01)\n",
    "\n",
    "for inp in xor_inputs:\n",
    "    print(mlp.predict(inp, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e31e3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_matrix:\n",
    "    def __init__(self, npl: List[int]): # neurons per layer, d in the slides\n",
    "        self.npl = list(npl)\n",
    "        self.L = len(npl)-1  # last layer index\n",
    "        # weights weight of layer l going from neuron i to neuron j: W[l][i][j]\n",
    "        self.W = []\n",
    "        self.W.append(np.array([]))  # no weights for input layer, to be consistent with indexing (W[1] are weights going to layer 1)\n",
    "\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.W.append(np.random.uniform(-1, 1, (self.npl[l-1]+1, self.npl[l]+1)))  # +1 for bias neuron from layer l-1 to layer l\n",
    "            self.W[l][ :, 0] = 0.0  # no weights going to bias neuron so set to 0.0, j == 0 corresponds to bias neuron\n",
    "\n",
    "        self.X = []  # activations of layer l neuron i: X[l][i]\n",
    "        self.deltas = []  # deltas of layer l neuron i: deltas[l][i]\n",
    "\n",
    "        for l in range(self.L + 1):\n",
    "            self.X.append(np.zeros((self.npl[l] + 1, 1)))\n",
    "            self.X[l][0][0] = 1.0  # bias neuron\n",
    "\n",
    "            self.deltas.append(np.zeros((self.npl[l] + 1, 1)))\n",
    "\n",
    "    def _propagate(self, inputs: np.ndarray, is_classification: bool):\n",
    "        assert(len(inputs) == self.npl[0]) # check if inputs is the same size as nbs of layers\n",
    "\n",
    "        self.X[0][1:] = inputs # update the X row vector for input layer (keeping bias neuron)\n",
    "\n",
    "        # update all layers until output\n",
    "        for l in range(1, self.L + 1):\n",
    "            signal = self.W[l].T @ self.X[l - 1]  # matrix multiplication\n",
    "\n",
    "            if (is_classification or l != self.L):\n",
    "                self.X[l] = np.tanh(signal)\n",
    "            else:\n",
    "                self.X[l] = signal # use identity method only when regression and updating last layer\n",
    "\n",
    "    def predict(self, inputs: np.ndarray, is_classification: bool):\n",
    "        self._propagate(inputs, is_classification)\n",
    "        return self.X[self.L][1:]\n",
    "    \n",
    "    def train(self,\n",
    "              X: np.ndarray,\n",
    "              Y: np.ndarray,\n",
    "              is_classification: bool,\n",
    "              num_iter: int,\n",
    "              alpha: float):\n",
    "        \n",
    "        assert(X.shape[1] == Y.shape[1])\n",
    "\n",
    "        for _ in tqdm(range(num_iter)):\n",
    "            k = np.random.randint(0, X.shape[1]) # pick a random sample\n",
    "            X_k = X[:,k].reshape(-1, 1) # get the k-th sample as a column vector\n",
    "            Y_k = np.ones((Y.shape[0]+1, 1))\n",
    "            Y_k[1:] = Y[:,k] # add bias neuron at index 0 (easy to handle with matrix operations)\n",
    "\n",
    "            self._propagate(X_k, is_classification) # mise à jour des sorties de tous les neurons (self.X)\n",
    "\n",
    "            self.deltas[self.L] = self.X[self.L] - Y_k\n",
    "\n",
    "            if is_classification:\n",
    "                self.deltas[self.L] *= (1.0 - self.X[self.L]**2) # square element wise\n",
    "\n",
    "            # get the deltas\n",
    "            for l in reversed(range(2, self.L + 1)):\n",
    "                self.deltas[l-1] = (1.0 - self.X[l-1]**2) * (self.W[l] @ self.deltas[l])\n",
    "\n",
    "            # update weights\n",
    "            for l in range(1, self.L + 1):\n",
    "                self.W[l] -= alpha*self.X[l-1]*self.deltas[l].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bf200125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.],\n",
      "       [0.],\n",
      "       [0.]]), array([[1.],\n",
      "       [0.],\n",
      "       [0.]]), array([[1.],\n",
      "       [0.]])]\n",
      "[array([], dtype=float64), array([[ 0.        , -0.19204588, -0.6227204 ],\n",
      "       [ 0.        ,  0.87395008, -0.31256436],\n",
      "       [ 0.        ,  0.17242477,  0.30763333]]), array([[ 0.        , -0.87925168],\n",
      "       [ 0.        ,  0.30000212],\n",
      "       [ 0.        ,  0.07860263]])]\n",
      "[array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.]])]\n"
     ]
    }
   ],
   "source": [
    "mlp_mat = MLP_matrix([2, 2, 1])\n",
    "print(mlp_mat.X)\n",
    "print(mlp_mat.W)\n",
    "print(mlp_mat.deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8c5e4e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36067352]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_mat.predict(np.array([[42.0], [53.0]]), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "872fadfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10004909]]\n",
      "[[0.11963283]]\n",
      "[[-0.0298551]]\n",
      "[[0.16280006]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:03<00:00, 27370.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.97646407]]\n",
      "[[0.97650081]]\n",
      "[[0.98403948]]\n",
      "[[-0.97297899]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "xor_inputs = np.array([\n",
    "    [0.0, 1.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0, 1.0],\n",
    "])\n",
    "\n",
    "xor_expected_ouputs =np.array([\n",
    "    [-1.0, 1.0, 1.0,-1.0]\n",
    "])\n",
    "\n",
    "for col in range(xor_inputs.shape[1]):\n",
    "    inp = xor_inputs[:, col].reshape(-1, 1)\n",
    "    print(mlp_mat.predict(inp, True))\n",
    "mlp_mat.train(xor_inputs, xor_expected_ouputs, True, 100000, 0.01)\n",
    "\n",
    "for col in range(xor_inputs.shape[1]):\n",
    "    inp = xor_inputs[:, col].reshape(-1, 1)\n",
    "    print(mlp_mat.predict(inp, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e840a24d",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- tester avec régression\n",
    "- implem en C++\n",
    "- changer vecteur d'état Snake\n",
    "- implémenter recorder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
